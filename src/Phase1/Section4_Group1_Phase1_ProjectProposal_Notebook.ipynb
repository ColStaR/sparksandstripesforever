{"cells":[{"cell_type":"markdown","source":["# Team 13 Phase 1 Project Proposal\n## Section 4, Group 1\n## Sparks and Stripes Forever\nNashat Cabral, Deanna Emery, Nina Huang, Ryan S. Wong\n\n[DataBricks Notebook Link](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/364123876153624/command/4295587629775265)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0fddeaf-0964-4529-a1f9-ea3049e0684b"}}},{"cell_type":"markdown","source":["## Phase Leader Plan\n\nPer the instructions, each weekly phase will be led by a different member of the team. Below is a table showing the planned leader for each of the upcoming phases.\n\nhttps://docs.google.com/spreadsheets/d/1Va1bwlEmrIrOc1eFo1ySYlPQpt4kZjDqahQABgw0la4/edit#gid=0\n\n\n| Phase Number | Phase Leader    |\n| ------------ | --------------- |\n| Phase 1      | Ryan S. Wong    |\n| Phase 2      | Nashat Cabral   |\n| Phase 3      | Deanna Emery    |\n| Phase 4      | Nina Huang      |\n| Phase 5      | Team Submission |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4c5212f-fdc4-4a0c-839f-1571ba03341a"}}},{"cell_type":"markdown","source":["## Credit Assignment Plan\n### Phase 1 Contributions\n\n| Task                                                   | Estimated Time (hours) | Nashat Cabral | Deanna Emery | Nina Huang | Ryan S. Wong |\n| ------------------------------------------------------ | ---------------------- | ------------- | ------------ | ---------- | ------------ |\n| Project Abstract Section                               | 1                      | X             |              | X          |              |\n| Data Overview Section                                  | 4                      | X             |              |            |              |\n| The Desired Outcomes and Metrics Section               | 2                      |               |              |            | X            |\n| Data Ingesting and Pipeline Section                    | 4                      |               |              |            | X            |\n| Joining Datasets Section                               | 4                      |               | X            |            |              |\n| Machine Learning Algorithms to be Used Section         | 2                      |               | X            |            |              |\n| Resource Management & Performance Optimization Section | 4                      |               |              | X          |              |\n| Train/Test Data Splits Section                         | 2                      |               |              | X          |              |\n| Conclusions and Next Steps Section                     | 2                      | X             |              | X          |              |\n| Open Issues or Problems Section                        | 2                      | X             |              | X          |              |\n| Set up Databricks instance                             | 2                      |               |              |            | X            |\n| Set up GitHub and Integrate with Databricks            | 1                      |               |              |            | X            |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf7f143-a8dc-4a8b-823a-d354b73dafd3"}}},{"cell_type":"markdown","source":["# Flight Delay Prediction Model Project\n\n## Project Team: Sparks and Stripes Forever\n\nNashat Cabral, Deanna Emery, Nina Huang, Ryan S. Wong"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff3ef503-8daf-44fc-9964-e4ef3762d541"}}},{"cell_type":"markdown","source":["## Project Abstract\n\nIn the flight industry, delays are a key issue for airline companies, airports, and customers. These events account for significant financial and time losses across these groups. This project seeks to predict the occurrence, or lack thereof, a delayed departing flight by using airport, flight, and local weather data to create a machine learning model that can effectively predict flight delays. Any analyses and methods applied will come from the perspective of benefiting the customer, and thus the model would need to place greater emphasis on correctly identifying non-delayed flights, as incorrectly identifying these events would be detrimental to the customer experience. Such a model would be capable of minimizing costs while improving customer satisfaction in their business. This document outlines the high level plan for approaching this problem."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e6b325-7a2e-4e0f-bf6c-2a73b5232722"}}},{"cell_type":"markdown","source":["## The Data\n\nFor this project, we will be examining the data from two tables of considerable size, along with an additional smaller table:\n\n- Flight Data: The first of the larger tables will consist of a collection of the “on-time” performance data for passenger flights from the U.S. Department of Transportation (DOT). These flights will be limited to domestic flights within the United States and its territories. \n- Weather Data: The second of the larger tables is composed of weather data that can be used to determine the effect of weather conditions on flight performance for airports within the region. \n- Airport Data: The final, smallest, table houses metadata on airports, such as their location and ID. \n\nBy exploring these datasets in conjunction with one another, we hope to develop a better understanding of the fields within and their possible relationships to one another. The information on the datasets below is obtained from the dataset documentation provided, as well as preliminary analyses on fields of interest.\n\nThe Airline On-Time Performance Data table contains the scheduled and actual departure/arrival times for U.S. Domestic flights for qualifying airline carriers. These carriers must account for at least one percentage of U.S Domestic scheduled passenger revenues in order to qualify. Our data ranges from 2015 to 2021, for the purposes of this preliminary analysis of the data, we will be examining the data from “/parquet_airlines_data_3m/”  which consists of flight data from the first quarter of 2015. In this study, canceled flights will be considered with the same regard for customers as delayed flights. Variables of interest within this dataset include: \n\n- ORIGIN_AIRPORT_ID- Identifier for the airport of departure\n- DEST_AIRPORT_ID- Identifier for the airport of arrival\n- FL_DATE- scheduled flight date \n- DEP_DELAY_NEW- numerical variable, difference in minutes between scheduled and actual departure time with early departures are set to 0\n- DEP_DEL15- binary categorical variable that indicates if a flight departure was delayed by more than 15 minutes\n- ARR_DELAY_NEW-  numerical variable, difference in minutes between scheduled and actual arrival time, early arrivals are set to 0\n- ARR_DEL15- binary categorical variable that indicates if a flight arrival was delayed by more than 15 minutes\n- CANCELLED- binary categorical variable indicating whether flight was canceled \n- DIVERTED- binary categorical variable indicating whether flight was diverted\n- CARRIER_DELAY - numerical variable, indicates time spent delayed due to carrier\n- WEATHER_DELAY - numerical variable, indicates time spent delayed due to weather\n- NAS_DELAY - numerical variable, indicates time spent delayed due to National Air System\n- SECURITY_DELAY - numerical variable, indicates time spent delayed due to security\n- LATE AIRCRAFT DELAY - numerical variable, indicates time spent delayed due to a late aircraft\n- ORIGIN_AIRPORT_ID, DEST_AIRPORT_ID, and FL_DATE will likely be combined to create a composite key as a unique identifer for each scheduled flight.\n\nThe below two figures display summary statistics for our numeric variables, as well as null value counts for our chosen variables.\n\n![img1](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/airlinestats.PNG)\n\n![img2](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/airlinenull.PNG)\n\nNull values shown above may indicate flights without delays, but will likely need to be removed/modified moving forward.\n\nThe Quality Controlled Local Climatological Data contains summary data from weather stations housed at airports. These stations log daily temperature highs/lows, precipitation, wind speed, visibility, and storm characteristics. The available data ranges from 2015 to 2021, for the purposes of this preliminary analysis of the data, we will be examining the data from “/parquet_weather_data_3m/”  which consists of weather data from the first quarter of 2015. (expand on variables). Variables of interest within this dataset are any that may have a relationship with flight delays, such as: \n\n- Station - identifier for each station\n- Date - Year-Month-Day-Hour-Minute-Second identifier for the date of a record, the field providing data to the hour allows for the field to identify hourly data.\n- HourlyWindSpeed - numerical variable, indicates wind speed in meters per second, 9999’s are considered missing values.\n- HourlySkyConditions  - Height in meters of the lowest cloud or obscuring item (max of 22,000)\n- HourlyVisibility - numerical variable, distance in meters an object can be seen (max of 16000), 999999 is considered missing\n- HourlyDryBulbTemperature - numerical variable, temperature of air in celsius, +9999 is considered missing\n- HourlySeaLevelPressure - numerical variable, air pressure relative to Mean Sea Level in hectopascals, 99999 is considered missing\n- Station and Date will likely be combined into a composite key as a unique identifier for each weather record.\n\nThe below two figures display summary statistics for our numeric variables, as well as null value counts for our chosen variables.\n\n![img3](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/weatherstats.PNG)\n\n![img4](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/weather%20null.PNG)\n\nThe statistics table shows the max being 9s, which is likely representative of missing data.\nThe figure for null values above indicates a large portion of data is missing from our dataset, these values may negatively affect any attempted analyses and will likely need to be filtered out.\n\n\nThe final table, stations_data, houses valuable information on airport location including fields such as: \n- lat - latitude\n- lon - longitude \n- station_id - identifier for each station\n- Distance_to_neighbor - numeric variable, distance to neighboring station in meters  \n- station_id will likely be used as the unique identifier for records within this dataset.\n\nThe below figure displays summary statistics for our numeric variables of Distance_to_neighbor\n\n![img5](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/stationstats.PNG)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddca6692-01cb-4f6e-a0c2-74b53024d99f"}}},{"cell_type":"markdown","source":["## The Desired Outcomes and Metrics\n\nOur desired outcome is to create a machine learning model that will effectively predict what flights will be categorized as delayed. The ability to predict what flights will not arrive on time is highly valuable to individual air travelers. To this intended audience, the ability to accurately predict flight delays will allow them to better economy their time and improve travel scheduling. Recall that within the data set in question, the features “ArrDel15” and “DepDel15” having a value of 1 indicates that the flight had been delayed.\n\nFor evaluating the effectiveness of our model, it is important that we kept the audience’s intended usage for the model in mind. When predicting flight delays, a false positive prediction is far worse than a false negative outcome: a flight that is predicted to be on-time but arrives late means the flier will have to wait, but a flight that is predicted to be late but arrives on-time means the flier will miss their flight entirely. For that reason, our primary metrics for success will be the F1 and precision scores of the model, with the model’s accuracy being a secondary metric. F1 and precision were both chosen as our primary metrics due to their emphasis on minimizing false positive occurrences while still tracking the positive predictive capabilities of the model. Accuracy is considered to be a secondary metric, as a high accuracy score is not as useful for the business case as F1 and precision , but accuracy does provide a good baseline measure of success for overall predictive ability without overfitting. \n\n\nF1, precision, and accuracy are described further with their equations in the bullet points below.\n\n- F1 is the harmonic mean of precision and recall, and is computed using the formula below.\n\n\\\\( F_1 = 2 * \\frac{precision \\cdot recall}{precision + recall} = \\frac{2 * TP}{2 * TP + FP + FN} \\\\)\n\n- Precision is the rate of true positivity within the model. It is computed using the formula below, where TP = true positives and FP = false positives.\n\n\\\\( Precision  = \\frac{TP}{TP + FP} \\\\)\n\n- Accuracy is the rate of all correctly classified observations. It is computed using the formula below, where TP = true positives, TN = true negatives, FP = false positives, and FN = false negatives.\n\n\\\\( Accuracy  = \\frac{TP + TN}{TP + TN + FP + FN} \\\\)\n\nFor creating a proper comparison and target for our model, we will compare our model against a simple baseline model. This baseline will be a model that predicts that every flight will not be delayed. Any improvements in our model over this baseline model will represent an overall improvement in the ability to correctly predict what flights will be delayed. Therefore, our desired model would be a model that has a high F1 and precision score while matching or exceeding the accuracy score of the baseline evaluation.\n\nThis desired outcome may lead to some interesting alternatives that would be worthwhile to explore as well. One possibility is to change the predicted value from being a categorical one to a numerical one, and focus our efforts towards getting accurate predictions of flight delays. This alternative would require measuring different metrics for success and incorporating different models, but it would be able to be done with the same data. The intended audience and use case would be the same as well."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892385be-8e23-4e75-9c76-816809f38d40"}}},{"cell_type":"markdown","source":["## The Data Ingesting and Pipeline\n\nThe raw data in question comes from multiple data sets on third party servers while being multiple gigabytes in size. In order for the data to be made usable, we will establish a data pipeline that conducts the necessary tasks to ingest, clean, transform, join, and make available the data we are interested in. To create this data pipeline, we will be leveraging the ML Pipelines API library, since that library is designed for machine learning pipelines and integrates natively with Apache Spark on our DataBricks instance.\n\nThe general process for data ingestion and processing through the pipeline is currently planned to take the following steps:\n1. Import data into HDFS\n    - The data in question is currently only available on a set of third party servers. We will first copy the raw data files from their current location into our Microsoft Azure  HDFS instance, uncompressing them so that they are fully accessible. \n2. Convert CSV into Parque\n    - Once stored within our cloud storage, we will convert the raw files from their native CSV format into the Parquet file format, as Parquet provides superior storage and computation performance compared to CSV. A copy of this converted raw data in Parque format will then be stored back onto our HDFS instance, which will then be transferred to our DataBricks instance for additional transformations.\n3. Clean data\n    - As with all raw data, we are expecting the initial data to contain numerous errors, typos, and missing data. To resolve this, the data will be cleaned of erroneous or troublesome values, and properly formatted so that it is fully usable. \n4. Transform data: outlier handling, scaling data, rebalancing data, adding features, join data sets, choosing features\n    - With the data cleaned, we will then transform the data to be usable for our purposes. \n    - We will start by handling outliers. Depending on what patterns we observe with features containing outliers, we will implement different treatment techniques such as dropping the record and imputing values. \n    - Next we will perform data scaling by applying normalized scaling to features that have a highly variable range of values. Our initial EDA of each feature’s distributions will reveal which features have data ranges that should be normalized.\n    - Afterwards, we will balance any features whose distributions are not normally distributed using the Synthetic Minority Oversampling Technique. SMOTE will allow us to artificially rebalance feature distributions, introducing slight noise but greatly increasing our analytical accuracy. An initial EDA will show what features have highly imbalanced distributions that would need to be rebalanced. \n    - We will also add additional derived features from the data that would be useful for our analysis. Lastly, we will be joining relevant data sets together. More information about joining the data sets can be found in the following section.\n    - In order to reduce storage and computational complexity, we will select the features in our data set that are most relevant to our analysis; all other features will be dropped from our usable data sets. We will be determining what features to keep and which to ignore after an initial EDA assessment of the data sets.\n5. Model Training\n    - After the data is imported, cleaned, and transformed successfully, the data is now ready to be used in our analysis. After saving the completed data sets to our HDFS instance, we will begin training our models in our DataBricks instance using Apache Spark. To minimize the amount of variance within the training set, we will be utilizing cross validation during our training and testing phases. More information about our cross validation methodology can be found in the “Train/Test Data Splits” section below. More information about the machine learning algorithms being used can be found in the section, “Machine Learning Algorithms to be Used” below.\n6. Model Evaluation\n    - After the data is trained, we will apply our model against our training data set. As mentioned previously, we will be using cross validation for our test data set. For more information about what evaluation metrics will be used, please refer to the section, “The Desired Outcomes and Metrics”.\n7. Model Re-Training and Hyperparameter Tuning\n    - After each iteration of training and evaluation, we will analyze and compare the performance of each model with the intention of improving our models’ desired metrics on the testing data. Each model will be retrained with different hyperparameters in order to increase our models’ performance.\n\nBelow is a visualization of the pipeline’s components and processes.\n\n![pipeline](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/Group1_Phase1_ProjectProposal_PipelineViz-1.jpg)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f23f4a4f-6193-4c7d-8aac-4d9a6ec90d8e"}}},{"cell_type":"markdown","source":["## Joining Datasets\n\nBefore we can begin feature engineering and machine learning, we first have to join weather and airlines datasets and address missing values. Since our goal is to predict the departure delay, we will have to drop any rows that are missing values in all three of DEP_DELAY, DEP_DELAY_NEW, and DEP_DEL15 (if any one of these columns is available, we will have sufficient information for our outcome variable). Similarly, if we have a small proportion of rows that are missing any important features, these can likely safely be dropped. Doing so can help improve runtime once we begin joining the datasets.\n\nThe steps below detail how the join of the airlines and weather datasets can be conducted:\n1. First, collect distinct pairs of airport codes from the ORIGIN_AIRPORT_ID column, and ORIGIN_STATE_ABR.\n2. Confirm that all airport codes and their corresponding states are in the weather stations dataset (this information can be found in the neighbor_call and neighbor_state columns). Perform an inner join on these two fields such that we have a table containing: ORIGIN_AIRPORT_ID (neighbor_call), ORIGIN_STATE_ABR (neighbor_state), station_id, distance_to_neighbor. Confirm that all airport codes and states were matched to at least one weather station, and that all matched weather stations exist in the weather data.\n3. Create a new column called WEATHER_DATETIME by converting the DATE column in the weather data to a datetime type.\n4. Perform an inner join of the airport - weather station linking table onto the weather dataset. The resulting dataset will only contain weather data for the stations of interest, and it may contain duplicate entries of weather if the same weather station corresponds to more than one airport. We may also find that one airport may correspond to more than one weather station.\n5. Identify rows in the merged weather data with duplicated WEATHER_DATETIME and airport (ORIGIN_AIRPORT_ID). Address these duplicates by keeping the row with the smallest value in “distance_to_neighbor”. For any ties, just keep the first occurrence. This strategy allows us to keep more granular time in our weather data by making use of multiple stations.\n6. Create a departure date-time column, called AIRLINES_DATETIME, in the airlines dataset by combining the FL_DATE, and departure time (CRS_DEP_TIME) into a datetime. Add a column called AIRLINES_DATETIME_SHIFTED to the airlines dataset that is equal to the departure datetime minus 2 hours. We will need to round the time down to the hour to help make merging onto the weather data easier, which can be accomplished by removing the last two digits from the departure time before converting to a datetime.\n7. Finally, we can merge the airlines data and the weather data together as a one-sided (left) merge onto the airlines data using the columns: ORIGIN_AIRPORT_ID, AIRLINES_DATETIME_SHIFTED, and WEATHER_DATETIME. The resulting table should have exactly as many rows as the original airlines dataset.\n\nWith all three datasets merged, we would need to identify the number and proportion of flights that did not have any corresponding weather data. If this is a small enough set, we can drop rows that are missing weather data. If we find that we need to keep these rows, we can fill the missing values with averages for the given ORIGIN_AIRPORT_ID and MONTH. Any other important features with missing values can be handled similarly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c7c7e95-0621-453a-833c-32bec0ed043a"}}},{"cell_type":"markdown","source":["## Machine Learning Algorithms to be Used\n\nAs a basic benchmark, we assume no flights will be deplayed. \nAs time permits, we will develop a more sophisticated benchmark: we can predict that a flight will be delayed if the airlines and airport have had any other delays prior in the day. To create this prediction, we can create an hourly cumulative count of delayed flights for each airline and airport over the course of a day with a lag of 2 (in order to avoid data leakage). We can then convert this shifted cumulative sum to a binary feature of ‘greater than 0’ or ‘equal to 0’. \n\nOur goal will be to develop a machine learning algorithm that outperforms our benchmark. In selecting our machine learning algorithms, we weigh in heavily on their ability to parallelize. For example, while certain models are great candidates for time series （such as LSTM), given that they can not be parallelized we will not consider them for the project. As such, we will attempt to implement the following algorithms using PySpark’s MLlib library:\n\n- [Logistic Regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression)\n\n  - The logistic regression is one of the most simplistic classification models and always a good place to start. It is bounded between 0 and 1, represented as: \\\\( f(\\bold{x}) = \\frac{1}{1 + e^{-\\bold{w}^T\\bold{x}}} \\\\)\n  - Its output can be interpreted as a likelihood, which gives us the flexibility to define a cutoff likelihood for positive and negative predictions. This means that we can optimize our model for precision or recall as desired.\n\n  - The loss function for logistic regression is defined as: \\\\( L(\\bold{w}; \\bold{x},y) = \\log(1 + \\exp(-y\\bold{w}^T\\bold{x})) \\\\)\n    Where \\\\( \\bold{w} \\\\) is the trained weights, \\\\( \\bold{x} \\\\) is the input data, and \\\\( y \\\\) is the true outcome value.\n\n- [Linear SVM ](https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine)\n\n  - The linear support vector machine algorithm defines a linear decision boundary that best separates positive and negative classes. The loss function for SVM is the Hinge loss, which is defined as: \\\\( L(\\bold{w}; \\bold{x},y) = \\max(0, 1 - y\\bold{w}^T\\bold{x})  \\\\)\n    Where \\\\( \\bold{w} \\\\) is the trained weights, \\\\( \\bold{x} \\\\) is the input data, and \\\\( y \\\\) is the true outcome value. \n  - MLlib’s implementation performs L2 regularization by default and uses an OWLQN optimizer.\n\n- [Random Forest](https://docs.google.com/document/d/1ZCUOfiGdChziaCCqxihUFIBQIjL8mRaQNTz-vA0Fhk0/edit#)\n\n  - Random Forest is an ensemble model of decision trees. This ensemble approach helps reduce overfitting, which is a risk for decision tree models. Decision trees use a 0-1 loss function, which is just the proportion of predictions that are incorrect (similar to an accuracy score). \n  - In a distributed system, we can train each decision tree in parallel. \n\nAs time permits, we may also explore the multilayer perceptron classifier in MLlib."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52636b4a-3383-4b3e-b54c-97ed4297a844"}}},{"cell_type":"markdown","source":["## Resource Management and Performance Optimization\n\nWorking with a big data problem over Azure means that extensive computing resources will be required which could cause the project to quickly run over the budget (e.g. free Azure account credit) if resources are not carefully managed. This is even more true with certain time series machine learning models which requires multiple features to be engineered for sliding windows. To avoid unexpected budget increase, the following best practices will be followed throughout the project:\n- Improving speed through memory: cache after major execution (e.g. after every data pipeline milestone including performing join and transforming data into training ready format)  to speed up processing. This can be achieved using the cache operation. Cached data will be cleared as the cluster gets destroyed.\n- Improving reliability by persist to disk: Persist or checkpoint to blob storage after major execution for reliability in case of unexpected reader disconnect\n  - Persist after performing data join - do not unpersist \n  - Optionally unpersist for other milestones along the data pipeline if needed to free up the blob storage for cost minimization \n- Spark API selection: use Spark DataFrame as opposed to RDD\n- Continuous Improvement practice: Monitor memory and disk usage after every major data pipeline milestone to identify opportunities of code optimization along the data pipeline\n- Code consciously: choose spark operations carefully with performance in mind, such as\n  - Broadcast - broadcast variables and small lookup tables where applicable\n  - Minimize shuffling - choose ByKey operations carefully (e.g. use combineByKey or reduceByKey instead of groupByKey)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"313c7500-cfca-456d-913c-82c51931ed3d"}}},{"cell_type":"markdown","source":["## Train/Test/Validation Data Splits\n\nWith time series data, data leakage can compromise the model result if data is not split appropriately. In fact, the traditional random splitting approach will not work as it will break the temporal nature of the data. Instead, an 80-20 train-test split will be performed on data that is sorted chronologically such that 80% of the data will be the ‘older’ part of the dataset that makes up the training data vs. the 20% is the ‘newer’ testing dataset. \n\nWhile performing the training/test data set on the full dataset (2015-2021) is desirable, balancing completeness of data with computing resources is required. As such, the following data splitting techniques will be used:\n- Perform EDA on the full dataset to identify unusual temporal patterns for opportunities to exclude data from the problem. Examples of such unusual patterns may be excluding data that covers the period from when the pandemic started to when airport operations stabilized. We assume that operational lessons learned from the Pandemic can be replicated quickly by airports in the event of another unexpected event that could impact the global airline operations and therefore can consider dropping the reactive phase\n- Create mini-model on 2019 data and observe how the model would perform differently as the model expands to include other period:\n  - Initially conduct 80-20 split on 2019 with Jan to mid-Sep as the training data and mid-Spe-Dec as the testing data\n  - Continue to fine tune the data through cross validation by including more data into the model\n\nZooming in on cross validation, we will use the Blocked Time Series split technique. The Blocked Time Series split technique is a variation of the time series split technique that reduces data leakage characterized by adding margins between the training and the testing dataset at each fold and across folds to prevent the model from observing lag values which are used twice. A visual illustration and sample implementation of the blocked time series technique are shown in below ([reference](https://goldinlocks.github.io/Time-Series-Cross-Validation/#:~:text=Blocked%20and%20Time%20Series%20Split,and%20another%20as%20a%20response.)).\n\n![Blocking Time Series Split](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/BlockingTimeSeriesSplit.JPG)\n\n![Blocking Time Series Split Code](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/BlockingTimeSeriesSplitCode.JPG)\n\nEach fold from the blocked CV will follow the 80-20 split as discussed above. The number of folds (k) will be decided after conducting EDA to identify opportunities of leaving data out from the problem. 2 approaches for determining the folds will be evaluated after completing EDA:\n1. Fixed duration: each fold will be approximately the same size \n2. Varying duration: each fold will be a specific period that is representative of special events such as optimal operations (e.g. pre-pandemic), disrupted & recovery from disrupted operations (e.g. onset of and recovery from the Pandemic), stabilized operations after major disruption (e.g. the new stabilized operations)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5833a610-6c9c-4d49-9719-3a4bf89903dd"}}},{"cell_type":"markdown","source":["## Team Names and Photos\n\nSection, Group Number: Section 4, Group 1\n\nTeam Name: Sparks and Stripes Forever\n\nTeam Members:\n- Nashat Cabral (cabralnc96@berkeley.edu)\n- Deanna Emery (deanna.emery@berkeley.edu)\n- Nina Huang (ninahuang2002@berkeley.edu)\n- Ryan S. Wong (ryanswong@berkeley.edu)\n\n\n![group photo](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/Group_Photo.JPG)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"260f9df1-15cb-409c-be3c-3ef526652a27"}}},{"cell_type":"markdown","source":["## GANTT Chart for Project Timeline and Tasks\n\nThe following picture was taken from the GANTT chart that our team is using in the project management program Asana. This chart shows the tasks and schedules for each of the project phases along with major deadlines and milestones.\n\n![GANTT Phase Overview](https://raw.githubusercontent.com/ColStaR/sparksandstripesforever/main/Phase1/images/PhaseOverview.JPG)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc5ad101-8e4c-4084-a35a-79dc1d34fc41"}}},{"cell_type":"markdown","source":["## Project Credit Assignment Plan\n\nPer the instructions, a full table of each team member’s assigned tasks have been provided in table format. Our initial planned version of the table is given below. As the project progresses, adjustments to the plan will be made on the table at the following link:\n\nhttps://docs.google.com/spreadsheets/d/1A4N3sV1ngaBsPdUqcJ8R4gY6e2g3gHeIUDyDes7f4SU/edit#gid=0\n\n| Phase Number | Team Member   | Tasks Completed                                                                                                                                                                                                                                   |\n| ------------ | ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Phase 1      | Nashat Cabral | Explain Data Section<br>Initial EDA and visuals<br>Data Pipeline plan section<br>Initial Pipeline Creation                                                                                                                                        |\n|              | Deanna Emery  | Join plan for data sets<br>Initial EDA and visuals<br>Model selection discussion section<br>Initial Pipeline Creation                                                                                                                             |\n|              | Nina Huang    | Strategizing resource management section<br>Train/Validation/Test split section<br>Data Pipeline plan section<br>Initial Pipeline Creation                                                                                                        |\n|              | Ryan S. Wong  | Outcome and metrics section<br>Data ingesting section<br>Phase leader plan<br>Credit assignment plan<br>GANTT chart<br>Initial Pipeline Creation                                                                                                  |\n| Phase 2      | Nashat Cabral | Plans to contribute to building the pipeline for the data<br>Focus on data cleaning tasks<br>Implement parallel training pipeline<br>Implement basic model usage.<br>Efficiency checks to ensure efficient code in Spark.                         |\n|              | Deanna Emery  | Plans to contribute to building the pipeline for the data<br>Focus on data cleaning tasks<br>Establish proper proper data set joins for data of interest.<br>Implement basic model usage.<br>Efficiency checks to ensure efficient code in Spark. |\n|              | Nina Huang    | Plans to contribute to building the pipeline for the data<br>Focus on data cleaning tasks<br>Establish proper proper data set joins for data of interest.<br>Implement basic model usage.<br>Efficiency checks to ensure efficient code in Spark. |\n|              | Ryan S. Wong  | Plans to contribute to building the pipeline for the data<br>Focus on data cleaning tasks<br>Implement parallel training pipeline<br>Implement parallel scoring pipeline<br>Efficiency checks to ensure efficient code in Spark.                  |\n| Phase 3      | Nashat Cabral | Plans to investigate influence levels for currently selected features<br>Tuning and experimentation of hyperparameters per model<br>Efficiency checks to ensure efficient code in Spark.                                                          |\n|              | Deanna Emery  | Plans to investigate influence levels for currently selected features<br>Tuning and experimentation of hyperparameters per model<br>Efficiency checks to ensure efficient code in Spark.                                                          |\n|              | Nina Huang    | Research optimal hyperparameters per model<br>Tuning and experimentation of hyperparameters per model<br>Efficiency checks to ensure efficient code in Spark.                                                                                     |\n|              | Ryan S. Wong  | Plans to investigate influence levels for currently selected features<br>Research optimal hyperparameters per model<br>Tuning and experimentation of hyperparameters per model                                                                    |\n| Phase 4      | Nashat Cabral | Plans to implement loss functions<br>Implement F1, Recall, Accuracy metrics<br>Establish baseline algorithm<br>Additional model testing and tuning.<br>Optimal model selection<br>Final report write-up                                           |\n|              | Deanna Emery  | Plans to implement loss functions<br>Additional model testing and tuning.<br>Optimal model selection<br>Final report write-up                                                                                                                     |\n|              | Nina Huang    | Plans to implement loss functions<br>Implement F1, Recall, Accuracy metrics<br>Establish baseline algorithm<br>Optimal model selection<br>Final report write-up                                                                                   |\n|              | Ryan S. Wong  | Plans to implement loss functions<br>Implement F1, Recall, Accuracy metrics<br>Establish baseline algorithm<br>Optimal model selection<br>Final report write-up<br>Final Presentation mock-up                                                     |\n| Phase 5      | Nashat Cabral | Class Presentation Preparation                                                                                                                                                                                                                    |\n|              | Deanna Emery  | Class Presentation Preparation                                                                                                                                                                                                                    |\n|              | Nina Huang    | Class Presentation Preparation                                                                                                                                                                                                                    |\n|              | Ryan S. Wong  | Class Presentation Preparation                                                                                                                                                                                                                    |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8acc0ac4-21f7-4187-a94e-85da5ad6494f"}}},{"cell_type":"markdown","source":["## Conclusions and Next Steps\n\nUpon reviewing the data and creating the macro solution (project pan), we would like to conclude our phase 1 analysis with the following summary:\n- Problem statement: empowering customers to plan their trip by providing if their flight will be delayed (either a ‘1’ in “ArrDel15” or “DepDel15”) by minimizing false negatives such that the customer can plan for their itinerary after their flight landed\n- Data pipeline: building efficient data pipelines by leveraging partitions with proper resource management techniques to solve the big-data, time-series, machine-learning problem feasible within the given budget (AWS free credit tier for Blob Storage). Key stages of the data pipeline include:\n  - Data format conversion: convert from csv to Parquet\n  - Joining datasets: bring together relevant datasets, which may include (depending on the result of the EDA)  the flight table, weather table, and airport table\n  - Data transformation: all the steps that are involved to prepare data into machine learning model ready format. Example of such steps include cleaning, scaling, rebalancing, feature engineering\n- Model training: train the data using training dataset and evaluate the model on the testing dataset. To avoid overfitting and perform parameter tuning, we will use the Blocked Time Series cross validation technique\n- Model result evaluation: model will be evaluated by balancing the recall and F1 to minimize false negatives while also considering prediction accuracy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43e9a74e-2cc6-4c2f-9044-89d4fa86b9fa"}}},{"cell_type":"markdown","source":["## Open Issues or Problems\n\nAs we are working with a big-data time series problem, optimizing computing resources such that running a model is technically feasible and operationally manageable will be a consistent problem. To address these problems, designing solutions to address the following open issues will continue to be focused on our subsequent phases:\n- Missing Data: Among our chosen fields of interest for this project, we have uncovered a large amount of missing data, particularly from the weather dataset. Going forward we will need to handle each of these circumstances with caution as the meaning of missing data may be of significance, or it may be a detriment to any conclusions we attempt to gather from the data.\n- Special period consideration: The data provided covers the Covid Pandemic outbreak (announced by the WHO on Jan 30, 2020). This means we may see segments of special periods with abnormal flight delay results, such as during the start of the pandemic, pandemic recovery, and stabilization after the pandemic. As such, historical data may not be sufficient to train a model that can respond to special periods. Creating a model to predict for these special periods will be a topic of continuous research and exploration\n- Training data time span: In addition to considering special periods, opportunities to further refine/reduce the size of the dataset required to create a machine learning model exists. Methods to refine the dataset include:\n  - Excluding data that may be no longer relevant - for example, does 2015 data help to provide 2021 result in light of the pandemic?\n  - Representing special periods - for example, should cross validation data splits be performed on special period phases?\n  - Parsimonious feature selection - keeping the number of features selected for the model to be lean (e.g no more than 25)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de1dad35-be6f-4b13-89b4-39fc67022207"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89f88cea-ea93-483c-95eb-cd8242105ccc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Group1_Phase1_ProjectProposal_Notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":364123876153624}},"nbformat":4,"nbformat_minor":0}
